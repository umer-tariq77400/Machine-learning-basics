{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Forward propagation](https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.linkedin.com%2Fpulse%2Fdemystifying-forward-propagation-neural-networks-real-world-v&psig=AOvVaw0CPn8hGRmz45qJSm_HMu5L&ust=1717158871659000&source=images&cd=vfe&opi=89978449&ved=0CBQQjhxqFwoTCIj5p7yxtYYDFQAAAAAdAAAAABAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with a neural network we see that I does training and after training it does predictions. Behind the scenes it does two things:  \n",
    "* [Forward propagation](https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.linkedin.com%2Fpulse%2Fdemystifying-forward-propagation-neural-networks-real-world-v&psig=AOvVaw0CPn8hGRmz45qJSm_HMu5L&ust=1717158871659000&source=images&cd=vfe&opi=89978449&ved=0CBQQjhxqFwoTCIj5p7yxtYYDFQAAAAAdAAAAABAE)  \n",
    "* Backward propagation  \n",
    "\n",
    "When we are making prediction from the model it only does forward propagation but when we are training a model it does both.  \n",
    "During training it first does forward propagation and then it does back propagation. Through backward propagation it can update the weights.  \n",
    "Backward propagation is a little complex so we will cover it in another tutorial but lets first look at the forward propagation.\n",
    "\n",
    "## [Forward propagation](https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.linkedin.com%2Fpulse%2Fdemystifying-forward-propagation-neural-networks-real-world-v&psig=AOvVaw0CPn8hGRmz45qJSm_HMu5L&ust=1717158871659000&source=images&cd=vfe&opi=89978449&ved=0CBQQjhxqFwoTCIj5p7yxtYYDFQAAAAAdAAAAABAE) (or forward pass) \n",
    "Forward propagation refers to the calculation and storage of intermediate variables (including outputs) for a neural network in order from the input layer to the output layer.\n",
    "\n",
    "You can see forward propagation in the image below \n",
    " \n",
    "![Forward Propagaton](https://media.licdn.com/dms/image/D5612AQE5LhqHLhJ2gg/article-cover_image-shrink_600_2000/0/1696319165807?e=2147483647&v=beta&t=6433lXyliKAbR75zYbOeeBjIy43vReRvnXphYLrl5XA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the numpy library(used for vector manipulation)\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input data\n",
    "x = np.array([200,17])\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our input for the model contains 2 values 200 and 17. So our input layer should also have 2 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the sigmoid function lets create the first layer of our neural network.  \n",
    "\n",
    "Our first layer neuron will contain 3 neurons. Every nueron will get the input data that we created ans every nueron will have its own parameters(weights, bias)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters for first neuron of first layer\n",
    "w1_1 = np.array([1,2])\n",
    "b1_1 = np.array([-1])\n",
    "\n",
    "# Computing the z1_1 and a1_1 using the dot product\n",
    "z1_1 = np.dot(w1_1,x) + b1_1\n",
    "a1_1 = sigmoid(z1_1)\n",
    "a1_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.45261912e-231])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters for second neuron fo first layer\n",
    "w1_2 = np.array([-3,4])\n",
    "b1_2 = np.array([1])\n",
    "\n",
    "# Computing the z1_2 and a1_2 using the dot product\n",
    "z1_2 = np.dot(w1_2,x) + b1_2\n",
    "a1_2 = sigmoid(z1_2)\n",
    "a1_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters for the third neuron of first layer\n",
    "w1_3 = np.array([5,-6])\n",
    "b1_3 = np.array([2])\n",
    "\n",
    "# Computing the z1_3 and a1_3 using the dot product\n",
    "z1_3 = np.dot(w1_3,x) + b1_3\n",
    "a1_3 = sigmoid(z1_3)\n",
    "a1_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+000],\n",
       "       [2.45261912e-231],\n",
       "       [1.00000000e+000]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now lets conbine the result from the first layer into a list that will be passed to the next layer(second layer in our case)\n",
    "a1 = np.array([a1_1,a1_2,a1_3])\n",
    "a1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create the second layer of our neural network. We will add only neuron in it.  \n",
    "It will take the input from the previous layer(which is a list of 3 values) and it will have 3 weights and 1 bias.  \n",
    "Then it will calculate a single value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99330715])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters for first neuron of second layer\n",
    "w2_1 = np.array([-7,8,9])\n",
    "b2_1 = np.array([3])\n",
    "\n",
    "# Computing the z2_1 and a2_1\n",
    "z2_1 = np.dot(w2_1,a1) + b2_1\n",
    "a2_1 = sigmoid(z2_1)\n",
    "a2_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the result from our forward propagation neural network\n",
    "\n",
    "Now lets sum it up by creating a function that will do all of this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 3), (1, 3))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = np.array([[ 1, -3,  5],\n",
    "              [ -2,  4, -6]])\n",
    "B = np.array([[-1,1,2]])\n",
    "W.shape, B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets impliment the forward propagation in a more general way rather than hardcoding the values\n",
    "def dense(a_in,W,b):\n",
    "    z = np.matmul(a_in, W) + B\n",
    "    a_out = sigmoid(z)\n",
    "    return a_out    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+000, 2.45261912e-231, 1.00000000e+000]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calling the function on the parameters and imput data that we have created.\n",
    "dense(x,W,B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank for reading.\n",
    "\n",
    "Untill next time  \n",
    "Allah Hafiz"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
